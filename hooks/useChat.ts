import { useState, useRef } from 'react';
import {
	ChatMessage,
	ChatConfig,
	QaConfig,
	SelectionMetadata,
	ScrollTarget,
} from '../types';
import { getChatResponseStream } from '../services/gemini';
import { compactHistory } from '../services/contextManager';
import { classifyError, delay } from '../services/errorClassifier';
import { processStream, createStreamUpdateHandler } from './useStreamProcessor';
import {
	MAX_AGENT_TURNS,
	handleFunctionCalls,
	buildToolResultMessage,
} from './useToolExecution';
import { LLMTransport } from '../services/llmTransport';
import { prompts } from '../services/prompts';
import { getTemplateById } from '../services/templateStorage';

interface UseChatProps {
	documentHtml: string;
	selectedText: SelectionMetadata | null;
	documentsContent: string[];
	qaConfig: QaConfig;
	chatConfig: ChatConfig;
	transport: LLMTransport;
	onDocumentEdit: (
		newHtml: string,
		reason: string,
		scrollTo?: ScrollTarget,
		scrollTargets?: ScrollTarget[]
	) => void;
}

interface UseChatReturn {
	messages: ChatMessage[];
	input: string;
	isLoading: boolean;
	isStreamingText: boolean;
	chatConfig: ChatConfig;
	setInput: (value: string) => void;
	setChatConfig: (config: ChatConfig) => void;
	handleSendMessage: (prompt?: string) => Promise<void>;
	handleEditMessage: (index: number, newContent: string) => Promise<void>;
	handleRetryMessage: (index: number) => Promise<void>;
	handleRetryAIMessage: (index: number) => Promise<void>;
	handleStopGeneration: () => void;
	handleResetChat: () => void;
	handleQuickGenerate: () => Promise<void>;
	removePlaceholderMessage: () => void;
	replacePlaceholderMessage: (message: ChatMessage) => void;
}

/**
 * Helper function to remove the placeholder message from the messages array
 */
const removePlaceholder = (messages: ChatMessage[]): ChatMessage[] => {
	const updated = [...messages];
	if (
		updated.length > 0 &&
		updated[updated.length - 1].role === 'model' &&
		updated[updated.length - 1].content === ''
	) {
		updated.pop();
	}
	return updated;
};

/**
 * Helper function to replace the placeholder message with a new message
 */
const replacePlaceholder = (
	messages: ChatMessage[],
	newMessage: ChatMessage
): ChatMessage[] => {
	const updated = [...messages];
	if (
		updated.length > 0 &&
		updated[updated.length - 1].role === 'model' &&
		updated[updated.length - 1].content === ''
	) {
		updated[updated.length - 1] = newMessage;
	} else {
		// Fallback: if placeholder not found, append (shouldn't happen)
		updated.push(newMessage);
	}
	return updated;
};

export const useChat = ({
	documentHtml,
	selectedText,
	documentsContent,
	qaConfig,
	chatConfig: initialChatConfig,
	transport,
	onDocumentEdit,
}: UseChatProps): UseChatReturn => {
	const [messages, setMessages] = useState<ChatMessage[]>([]);
	const [input, setInput] = useState('');
	const [isLoading, setIsLoading] = useState(false);
	const [isStreamingText, setIsStreamingText] = useState(false);
	const [chatConfig, setChatConfig] = useState<ChatConfig>(initialChatConfig);
	const abortControllerRef = useRef<AbortController | null>(null);

	const removePlaceholderMessage = () => {
		setMessages(removePlaceholder);
	};

	const replacePlaceholderMessage = (message: ChatMessage) => {
		setMessages((prev) => replacePlaceholder(prev, message));
	};

	const onStreamUpdate = createStreamUpdateHandler(setMessages);

	/** Wrapper around processStream that uses the hook's abort controller */
	const processChatStream = async (
		responseStream: AsyncGenerator<any, void, unknown>
	) => {
		return processStream(
			responseStream,
			abortControllerRef.current?.signal || null,
			onStreamUpdate
		);
	};

	/**
	 * Send a message and run the agent loop.
	 *
	 * Implements a unified loop: LLM â†’ tool calls â†’ execute â†’ feed result
	 * back â†’ repeat until the LLM responds with text only (no tool calls)
	 * or MAX_AGENT_TURNS is reached.
	 *
	 * This fixes the stale-HTML bug by tracking `latestHtml` across iterations.
	 */
	const sendMessageWithContext = async (
		messageToSend: string,
		contextMessages: ChatMessage[]
	) => {
		if (!messageToSend.trim()) return;

		const userMessage: ChatMessage = { role: 'user', content: messageToSend };
		const updatedMessages = [...contextMessages, userMessage];

		setIsLoading(true);
		setIsStreamingText(true);

		// Create new AbortController for this message
		abortControllerRef.current = new AbortController();

		// Set messages with user message and placeholder for model response
		setMessages([...updatedMessages, { role: 'model', content: '' }]);

		// Compact history before sending to the API (UI still shows all messages)
		let loopHistory = compactHistory(contextMessages);
		let latestHtml = documentHtml;
		let currentMessage = messageToSend;
		let iteration = 0;

		try {
			while (iteration < MAX_AGENT_TURNS) {
				iteration++;

				// --- Call LLM ---
				const responseStream = getChatResponseStream(
					loopHistory,
					currentMessage,
					documentsContent,
					latestHtml,
					selectedText,
					qaConfig.apiKey,
					chatConfig.model,
					qaConfig,
					abortControllerRef.current!.signal,
					transport
				);

				const streamResult = await processChatStream(responseStream);

				if (!streamResult.fullResponse) {
					removePlaceholderMessage();
					break;
				}

				// --- Process tool calls ---
				const result = await handleFunctionCalls(
					streamResult,
					latestHtml,
					transport
				);

				if (abortControllerRef.current?.signal.aborted) {
					const abortError = new Error('Aborted');
					abortError.name = 'AbortError';
					throw abortError;
				}

				// No function calls â€” normal text response, we're done
				if (!result.newHtml && !result.toolResponse && result.success) {
					break;
				}

				// --- Apply successful edit ---
				if (result.success && result.newHtml !== undefined) {
					onDocumentEdit(
						result.newHtml,
						messageToSend,
						result.scrollTo,
						result.scrollTargets
					);
					latestHtml = result.newHtml; // Fix stale HTML bug
				}

				// --- Check if this is a terminal tool result (successful edit with no more work needed) ---
				// For a successful edit, show the result and let the loop continue
				// so the LLM can decide if it needs to do more
				if (
					result.success &&
					result.newHtml !== undefined &&
					!result.toolResponse
				) {
					// Show success in chat but continue loop so LLM can respond
					setMessages((prev) => {
						const updated = [...prev];
						updated[updated.length - 1] = {
							role: 'model',
							content:
								result.toolUsageMessage || 'âœ… *Document updated successfully.*',
						};
						return updated;
					});
					break;
				}

				// --- Build tool result and feed back to LLM ---
				const toolResultMsg = buildToolResultMessage(
					result,
					iteration,
					MAX_AGENT_TURNS
				);

				// Show agent working status
				if (iteration < MAX_AGENT_TURNS) {
					setMessages((prev) => {
						const updated = [...prev];
						updated[updated.length - 1] = {
							role: 'model',
							content: `ðŸ”„ *Working... (step ${iteration}/${MAX_AGENT_TURNS})*`,
						};
						return updated;
					});
				}

				// Advance the conversation: add AI response + tool result as next turn
				loopHistory = [
					...loopHistory,
					{ role: 'user', content: currentMessage },
					{
						role: 'model',
						content: streamResult.accumulatedText || 'I processed the tool call.',
					},
				];
				currentMessage = toolResultMsg;

				// Reset placeholder for the next streaming response
				setMessages((prev) => {
					const updated = [...prev];
					updated[updated.length - 1] = { role: 'model', content: '' };
					return updated;
				});
			}

			// Max turns reached â€” show status (Continue button handled by UI)
			if (iteration >= MAX_AGENT_TURNS) {
				console.warn(
					`[useChat] Agent loop reached max turns (${MAX_AGENT_TURNS}).`
				);
				setMessages((prev) => {
					const updated = [...prev];
					const last = updated[updated.length - 1];
					if (last.role === 'model' && last.content === '') {
						updated[updated.length - 1] = {
							role: 'model',
							content: `âš ï¸ *Reached maximum steps (${MAX_AGENT_TURNS}). You can ask me to continue if there's more work to do.*`,
						};
					}
					return updated;
				});
			}
		} catch (error) {
			if (error instanceof Error && error.name === 'AbortError') {
				console.log('Chat aborted by user');
				removePlaceholderMessage();
				return;
			}

			const classified = classifyError(error);
			console.error(`[useChat] ${classified.type} error:`, error);

			// Auto-retry for transient errors
			if (classified.retryable && classified.retryDelayMs !== undefined) {
				// Show retry message to user
				replacePlaceholderMessage({
					role: 'model',
					content: classified.userMessage,
				});

				try {
					if (classified.retryDelayMs > 0) {
						await delay(classified.retryDelayMs);
					}

					// For context overflow, aggressively prune before retry
					const retryHistory =
						classified.type === 'context_overflow'
							? compactHistory(contextMessages).slice(-6) // Keep only last 3 turns
							: compactHistory(contextMessages);

					abortControllerRef.current = new AbortController();

					const retryStream = getChatResponseStream(
						retryHistory,
						messageToSend,
						documentsContent,
						latestHtml, // Use latest HTML, not stale documentHtml
						selectedText,
						qaConfig.apiKey,
						chatConfig.model,
						qaConfig,
						abortControllerRef.current.signal,
						transport
					);

					// Replace with a fresh placeholder for retry
					setMessages((prev) => {
						const updated = [...prev];
						updated[updated.length - 1] = { role: 'model', content: '' };
						return updated;
					});

					const retryResult = await processChatStream(retryStream);
					if (retryResult.fullResponse) {
						const result = await handleFunctionCalls(
							retryResult,
							latestHtml,
							transport
						);

						if (result.success && result.newHtml !== undefined) {
							onDocumentEdit(
								result.newHtml,
								messageToSend,
								result.scrollTo,
								result.scrollTargets
							);
							setMessages((prev) => {
								const updated = [...prev];
								updated[updated.length - 1] = {
									role: 'model',
									content:
										result.toolUsageMessage || 'âœ… *Document updated successfully.*',
								};
								return updated;
							});
						}
						// If no edit, the streamed text is already shown
					}
					return;
				} catch (retryError) {
					console.error('[useChat] Retry also failed:', retryError);
					// Fall through to show error
				}
			}

			// Non-retryable or retry failed â€” show classified error
			replacePlaceholderMessage({
				role: 'model',
				content: classified.userMessage,
			});
		} finally {
			setIsLoading(false);
			setIsStreamingText(false);
			abortControllerRef.current = null;
		}
	};

	const handleSendMessage = async (prompt?: string) => {
		const messageToSend = prompt || input;
		if (!messageToSend.trim()) return;

		setInput('');
		await sendMessageWithContext(messageToSend, messages);
	};

	const handleEditMessage = async (index: number, newContent: string) => {
		if (index < 0 || index >= messages.length) return;
		if (messages[index].role !== 'user') return;

		// Truncate messages up to and including the target message
		const truncatedMessages = messages.slice(0, index + 1);

		// Update the target message with new content
		truncatedMessages[index] = { role: 'user', content: newContent };

		// Send the updated message
		await sendMessageWithContext(newContent, truncatedMessages.slice(0, -1));
	};

	const handleRetryMessage = async (index: number) => {
		if (index < 0 || index >= messages.length) return;
		if (messages[index].role !== 'user') return;

		const messageContent = messages[index].content;

		// Truncate messages up to and including the target message
		const truncatedMessages = messages.slice(0, index + 1);

		// Send the message again (without the last message since it will be added in sendMessageWithContext)
		await sendMessageWithContext(messageContent, truncatedMessages.slice(0, -1));
	};

	const handleRetryAIMessage = async (index: number) => {
		if (index < 0 || index >= messages.length) return;
		if (messages[index].role !== 'model') return;

		// Find the previous user message (the one that generated this AI response)
		let userMessageIndex = -1;
		for (let i = index - 1; i >= 0; i--) {
			if (messages[i].role === 'user') {
				userMessageIndex = i;
				break;
			}
		}

		// If no previous user message exists, do nothing
		if (userMessageIndex === -1) return;

		const userMessageContent = messages[userMessageIndex].content;

		// Truncate messages up to (but not including) the AI message
		// This means we keep all messages up to and including the user message
		const truncatedMessages = messages.slice(0, userMessageIndex + 1);

		// Send the user message again (without the last message since it will be added in sendMessageWithContext)
		await sendMessageWithContext(
			userMessageContent,
			truncatedMessages.slice(0, -1)
		);
	};

	const handleStopGeneration = () => {
		if (abortControllerRef.current) {
			abortControllerRef.current.abort();
		}
	};

	const handleResetChat = () => {
		setMessages([]);
		setInput('');
	};

	const handleQuickGenerate = async () => {
		const selectedTemplate = qaConfig.selectedTemplateId
			? getTemplateById(qaConfig.selectedTemplateId)
			: null;
		const prompt = prompts.buildGenerationPrompt(
			qaConfig,
			selectedTemplate?.templateString
		);
		// Reset chat for a fresh generation
		setMessages([]);
		await sendMessageWithContext(prompt, []);
	};

	return {
		messages,
		input,
		isLoading,
		isStreamingText,
		chatConfig,
		setInput,
		setChatConfig,
		handleSendMessage,
		handleEditMessage,
		handleRetryMessage,
		handleRetryAIMessage,
		handleStopGeneration,
		handleResetChat,
		handleQuickGenerate,
		removePlaceholderMessage,
		replacePlaceholderMessage,
	};
};
